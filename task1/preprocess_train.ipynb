{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import * # Functions written by team ghostbusters\n",
    "import seaborn as sns\n",
    "from category_encoders import TargetEncoder\n",
    "import networkx as nx\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['listing_id', 'title', 'address', 'property_name', 'property_type',\n",
       "       'tenure', 'built_year', 'num_beds', 'num_baths', 'size_sqft',\n",
       "       'floor_level', 'furnishing', 'available_unit_types', 'total_num_units',\n",
       "       'property_details_url', 'lat', 'lng', 'elevation', 'subzone',\n",
       "       'planning_area', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Property type </h3>\n",
    "Ordinal encoding property_type based on EDA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generalize property type\n",
    "\"\"\"\n",
    "df= df_train.copy()\n",
    "df['general_property_type'] = df['property_type'].apply(generalize_property_type) # 3 general typesï¼š landed hdb condo\n",
    "\n",
    "\"\"\"\n",
    "Standardize property type\n",
    "\"\"\"\n",
    "df['property_type'] = df['property_type'].apply(standardize_property_type) # lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Property categori ordinal encoding\n",
    "The order of encoding follows the EDA results from small to large\n",
    "\"\"\"\n",
    "ordered_ppt_type = {\n",
    "\"hdb\":1, \n",
    "\"executive condo\":2, \"walk-up\":2,\"shophouse\":2,\n",
    "\"condo\":3,\"apartment\":3,\n",
    "\"townhouse\":4,\"terraced house\":4,\"landed\":4,\"cluster house\":4,\n",
    "\"corner terrace\":5,\n",
    "\"conservation house\":6,\n",
    "\"semi-detached house\":7,\n",
    "\"bungalow\":8,\n",
    "\"land only\":9,\n",
    " }\n",
    "df[\"property_type_ordinal\"] = df[\"property_type\"].replace(ordered_ppt_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Price</h3>\n",
    "Clean price outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Num before clean: 20254\n",
      "Data Num after clean 1: 20252\n",
      "Data Num after clean 2: 20151\n",
      "Data Num after clea 3: 20149\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Price data cleaning\n",
    "\"\"\"\n",
    "print(\"Data Num before clean:\", df.shape[0])\n",
    "#   1. Remove large price\n",
    "price_outlier = get_outlier(df,'price')\n",
    "df = df.drop(price_outlier)\n",
    "print('Data Num after clean 1:', df.shape[0])\n",
    "\n",
    "#   2. Remove 0 price\n",
    "df= df[df.price>0]\n",
    "print('Data Num after clean 2:', df.shape[0])\n",
    "\n",
    "#   3. Remove property type related noise price\n",
    "#   Threshold based on EDA\n",
    "hdb_outlier = df[(df.general_property_type=='hdb')&(df.price>2000000)].index\n",
    "df = df.drop(hdb_outlier)\n",
    "print('Data Num after clea 3:', df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Size Sqft</h3>\n",
    "Clean outlier, fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number after clean 20073\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clean size_sqft\n",
    "\"\"\"\n",
    "#   Remove large outlier, transfrom small outlier to size square feet if can find corresponding data in the same property\n",
    "df.size_sqft = df.apply(lambda r: standardize_size(r, df, 400, 6000,'hdb'), axis=1)\n",
    "df.size_sqft = df.apply(lambda r: standardize_size(r, df, 400, 1000000,'condo'), axis=1)\n",
    "df.size_sqft = df.apply(lambda r: standardize_size(r, df, 400, 60000,'landed'), axis=1)\n",
    "df = df.dropna(subset=['size_sqft'])    # Large outlier are set as na\n",
    "\n",
    "print('Number after clean', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tenure </h3>\n",
    "Group tenure, ordinal encoding based on EDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tenure\n",
    "\"\"\"\n",
    "# Genralize tenur into 3 groups 'Nan', '99~110 year' ,'900+ year', 'freehold'\n",
    "# Ordinal encoding following EDA results\n",
    "df['tenure_group'] = df['tenure'].apply(standardize_tenure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Numer of beds</h3>\n",
    "Fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null in num_beds: 68\n",
      "data num after cleaning: 20065\n",
      "Null in num_beds after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bed\n",
    "\"\"\"\n",
    "print('Null in num_beds:',len(df[df.num_beds.isnull()]))\n",
    "\n",
    "#   1. Set studio bed\n",
    "is_studio = df.title.str.contains('studio')# is studio\n",
    "is_null = df.num_beds.isnull() # num_beds is null\n",
    "is_small = df.size_sqft <=900 # is studio but not studio house\n",
    "df.loc[is_studio&is_null&is_small,\"num_beds\"] = 1\n",
    "\n",
    "#   2. Fill with information of the same property\n",
    "df['num_beds'] = df.groupby(['property_name','size_sqft'])['num_beds'].transform(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "#   3. Fill num_beds with hdb info, in comparison with other type of house, the floor plan and size of hdb is generally fiexed\n",
    "is_null = df.num_beds.isnull() # num_beds is null\n",
    "is_4rm = df.size_sqft > 1290 # Observation from EDA over 85% of 4 room larger than 1290 sqft\n",
    "is_2rm = df.size_sqft < 900 # Observation from EDA over 95% of 2 room smaller than 900 sqft\n",
    "is_hdb = df.general_property_type.str.contains(\"hdb\", na=False, case=False) # is hdb\n",
    "df.loc[is_hdb&is_null&is_4rm,'num_beds'] = 4\n",
    "df.loc[is_hdb&is_null&(~is_2rm)&(~is_4rm),'num_beds'] = 3\n",
    "df.loc[is_hdb&is_null&is_2rm,'num_beds'] = 2\n",
    "\n",
    "#   4. Drop invalid data \n",
    "df = df.dropna(subset=['num_beds'])\n",
    "print('data num after cleaning:',df.shape[0])\n",
    "print('Null in num_beds after cleaning:',len(df[df.num_beds.isnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Num_baths</h3>\n",
    "Fill missing values, drop outlier and other missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null in num_baths: 432\n",
      "data num after cleaning: 20012\n",
      "Null in num_baths after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "baths\n",
    "\"\"\"\n",
    "print('Null in num_baths:',len(df[df.num_baths.isnull()]))\n",
    "\n",
    "#   1. Fill with information of the same property\n",
    "df['num_baths'] = df.groupby(['property_name','num_beds','size_sqft'])['num_baths'].transform(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "#  2. Drop data that cannot be filled\n",
    "df = df.dropna(subset=['num_baths'])\n",
    "df[df.num_baths.isnull()].shape[0]\n",
    "print('data num after cleaning:',df.shape[0])\n",
    "print('Null in num_baths after cleaning:',len(df[df.num_baths.isnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Remove outliers regarding num_beds and num_baths</h3>\n",
    "Find outlier based on bed2bath ratio, fill outlier with reference value from same property, drop other outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  1. Try filling noisy data with information of the same property, outliers are defined by based on boxplot result\n",
    "df_noise = df.copy()\n",
    "df_noise['bed2bath'] =df_noise['num_beds']/df_noise['num_baths']\n",
    "df_noise_cleaned = df_noise.apply(lambda r: standardize_bednbath(r, df_noise, 0.4,3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data num before cleaning: 20012\n",
      "data num after cleaning: 20007\n",
      "Null in num_beds after cleaning: 0\n",
      "Null in num_baths after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "print('data num before cleaning:',df_noise.shape[0])\n",
    "df_noise_arr = np.array([*df_noise_cleaned])\n",
    "df_noise['num_beds'] = df_noise_arr[:,0]\n",
    "df_noise['num_baths'] = df_noise_arr[:,1]\n",
    "df_noise = df_noise.dropna(subset=['num_beds','num_baths'])\n",
    "print('data num after cleaning:',df_noise.shape[0])\n",
    "\n",
    "print('Null in num_beds after cleaning:',len(df_noise[df_noise.num_beds.isnull()]))\n",
    "print('Null in num_baths after cleaning:',len(df_noise[df_noise.num_baths.isnull()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Built Year</h3>\n",
    "Fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Built year \n",
    "\"\"\"\n",
    "##  Fill missing value based on mean values of each general property type 'hdb' 'condo' 'landed'\n",
    "df_year = df_noise.copy()\n",
    "year_group = df_year.groupby(by = ['general_property_type'])['built_year'].transform(lambda x: int(x.mean()))\n",
    "df_year['built_year'] = df_year['built_year'].fillna(year_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>price per sqft</h3>\n",
    "Calculate price every square feet using cleaned data, remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   1. Calculate size per sqft\n",
    "df_per_price = df_year.copy()\n",
    "df_per_price[\"per_price\"] = df_per_price[\"price\"]/df_per_price[\"size_sqft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   2. Remove noise\n",
    "df = df_per_price[df_per_price.per_price<30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Target encoding per price based on subzone</h3>\n",
    "Target encoding per_price attributes based on subzone category of data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Subzone name encode\n",
    "\"\"\"\n",
    "encoder = TargetEncoder()\n",
    "df['subzone_per_price_encoded'] = encoder.fit_transform(df['subzone'], df['per_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries 20003\n"
     ]
    }
   ],
   "source": [
    "print(\"number of entries\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Auxiliary Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MRT</h3>\n",
    "find 50 mrt stations with the highest in/out degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_mrt_connections = pd.read_csv('data/auxiliary-data/sg-mrt-connections.csv')\n",
    "df_mrt = pd.read_csv('data/auxiliary-data/sg-mrt-stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhoby ghaut (0.03968)\n",
      "macpherson (0.03175)\n",
      "little india (0.03175)\n",
      "buona vista (0.03175)\n",
      "chinatown (0.03175)\n",
      "['dhoby ghaut', 'macpherson', 'little india', 'buona vista', 'chinatown', 'botanic gardens', 'newton', 'serangoon', 'bugis', 'bishan', 'outram park', 'woodlands', 'promenade', 'paya lebar', 'tampines', 'raffles place', 'caldecott', 'expo', 'tanah merah', 'jurong east', 'bayfront', 'marina bay', 'city hall', 'tanjong pagar', 'bright hill', 'mayflower', 'mattar', 'rochor', 'one-north', 'kent ridge', 'downtown', 'telok ayer', 'ubi', 'dover', 'farrer park', 'boon keng', 'clarke quay', 'kaki bukit', 'farrer road', 'holland village', 'harbourfront', 'telok blangah', 'marsiling', 'kranji', 'sengkang', 'buangkok', 'stevens', 'bedok', 'yew tee', 'bukit batok']\n"
     ]
    }
   ],
   "source": [
    "# Find important statinons \n",
    "G_undirected = nx.Graph()\n",
    "\n",
    "for idx, row in df_mrt_connections.iterrows():\n",
    "    G_undirected.add_edge(row['to'], row['from'])\n",
    "\n",
    "# Use degree centrality\n",
    "nx_degree_scores = nx.algorithms.centrality.degree_centrality(G_undirected)\n",
    "\n",
    "ordered_degree_scores = sorted(nx_degree_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for station, score in ordered_degree_scores[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))\n",
    "\n",
    "important_mrt_stations = [entry[0] for entry in ordered_degree_scores[:50]]\n",
    "print(important_mrt_stations)\n",
    "\n",
    "df_important_mrt = df_mrt[df_mrt['name'].isin(important_mrt_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance cauculated based on lat and lng\n",
    "df = add_distance_to_nearest_mrt(df_important_mrt, df, 'dist_to_nearest_important_mrt')\n",
    "df['dist_to_nearest_important_mrt_rounded'] = df['dist_to_nearest_important_mrt'].round(0).astype(int) #Round to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Shopping Mall</h3>\n",
    "Find number of shopping malls within 300 m of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Load Data\n",
    "df_shopping_mall = pd.read_csv('./data/auxiliary-data/sg-shopping-malls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Add number of nearby shopping malls\n",
    "#   Within 300 m of the property\n",
    "df = add_number_of_nearby_shopping_malls(df_shopping_mall,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Schools</h3>\n",
    "Find number of nearby schools within 1 km of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_primary_schools = pd.read_csv('./data/auxiliary-data/sg-primary-schools.csv')\n",
    "df_secondary_schools = pd.read_csv('./data/auxiliary-data/sg-secondary-schools.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Within 1km of the property\n",
    "df = add_number_of_nearby_primary_schools(df_primary_schools, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Within 1km of the property\n",
    "df = add_number_of_nearby_secondary_schools(df_secondary_schools, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Commertial Centre</h3>\n",
    "Find name of nearest commercial centre(type: BN and CR) within 10 km of the property.\n",
    "Ordinal encoding the name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc = pd.read_csv('./data/auxiliary-data/sg-commerical-centres.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= add_name_of_nearest_commercial_centre_by_type(df,df_cc,'BN',10)# within 10 km\n",
    "df = add_name_of_nearest_commercial_centre_by_type(df,df_cc,'CR',10) # within 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Categorical encoding BN\n",
    "ordered_BN = {\n",
    "'Novena':9,\n",
    "'Alexandra':8,\n",
    " 'Buona Vista':7,\n",
    " 'Paya Lebar Central':6,'Serangoon':5,\n",
    " 'Bishan':4,\n",
    " 'Changi East Urban District':3,\n",
    " 'Tao Payoh':2,\n",
    " 'None':1\n",
    " }\n",
    "df[\"name_of_nearest_BN_ordinal\"] = df[\"name_of_nearest_BN\"].replace(ordered_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Categorical encoding CR\n",
    "ordered_CR = {\n",
    "'Central Business District':5,\n",
    " 'Jurong Lake District':4,\n",
    " 'Tampines Regional Centre':3,\n",
    " 'Seletar Regional Centre':2,\n",
    " 'Woodlands Regional Centre':1\n",
    " }\n",
    "df[\"name_of_nearest_CR_ordinal\"] = df[\"name_of_nearest_CR\"].replace(ordered_CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Drop features</h2>\n",
    "Only contains features that will be useful for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['built_year', 'num_beds', 'num_baths', 'lat', 'lng', 'size_sqft',\n",
    "                    'tenure_group', 'subzone_per_price_encoded',\n",
    "                    'property_type_ordinal',\n",
    "                    #mrt\n",
    "                    'dist_to_nearest_important_mrt_rounded',\n",
    "                    #schools\n",
    "                    'number_of_nearby_primary_schools', \n",
    "                    'number_of_nearby_secondary_schools', \n",
    "                    #shopping mall\n",
    "                    'number_of_nearby_shopping_malls',\n",
    "                    #CR\n",
    "                    'name_of_nearest_BN_ordinal',\n",
    "                    'name_of_nearest_CR_ordinal',\n",
    "                    #dependent variable\n",
    "                    'price',\n",
    "                    'per_price'\n",
    "                    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is still contains missing values\n",
    "for col in df_final.columns:\n",
    "    if df_final[col].isna().sum():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Save to CSV </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV without dropping columns, will be useful for target encoding of test set\n",
    "df.to_csv('./data/train_final_complete_nodrop.csv',index=False)\n",
    "# CSV for model training\n",
    "df_final.to_csv('./data/train_final_complete_nodrop.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cceb2fdb6d0d49712663a09c62496ac932ce62398c41e486b12dcc6178307130"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
